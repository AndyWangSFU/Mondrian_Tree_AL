Run sim_var and sim_py_linear. First it will print the following for each leaf:

Number currently in leaf
Number in leaf after active learning
Optimal number in leaf
The sides of that leaf.

It will then print 5 sets of MSE:

MSE for Mondrian Tree with active learning (mt_al)
MSE for Mondrian Tree with passive learning (mt_rn)
MSE for oracle Mondrian Tree (Mondrian tree but with no (little) noise in the leaf conditional mean estimates) (oracle)
MSE for Breiman Tree with active learning selected data from Mondrian Tree active learning (same labelled data as first) (bt_al)
MSE for Breiman Tree with passive learning (same labelled data as second) (bt_rn)

Most of the time in both simulations, mt_al is a little better than mt_rn. However the improvement is usually pretty small compared to the entire MSE, although it is usually a fairly significant proportion of the MSE that isn't oracle MSE. This is because oracle MSE is error from (bias^2 + noise). And for Mondrian trees this is the term which dominates the MSE since var = 1/n (bias^2 + noise), with n being n-per-leaf. So Mondrian trees are low variance high bias, and we are working to lower the variance. However Breiman trees are low bias high variance, and so by using the active learning scheme from Mondrian trees and then using Breiman trees as our final regressor we gain a lot compared to passive sampling. Indeed we see that the difference between active and passive is much larger for the bt than for the mt. Occasionally passive is still better than active, however this tends to be when the tree has grown in a strange way (try setting the seed=3 for an example of this).
